# -*- coding: utf-8 -*-
"""RandomForest

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_QsKWbmS79mYGJwPT6ky22BUtmgZogbi
"""

# Download your script (optional if you already did this)
!wget -O prepwork.py https://raw.githubusercontent.com/tasitasitasi/SPOTIFY/main/prepwork.py

# Download the CSV dataset into /content/
!wget -O /content/clean_spotify_post2012.csv https://raw.githubusercontent.com/tasitasitasi/SPOTIFY/main/clean_spotify_post2012.csv

# Quick sanity check: files present?
!ls -lh /content | grep -E "prepwork\.py|clean_spotify_post2012\.csv"

# Optional: preview the first few lines of the CSV
!head -n 5 /content/clean_spotify_post2012.csv

!sed -i "s|\.\./data/clean_spotify_post2012\.csv|/content/clean_spotify_post2012.csv|" prepwork.py

# Optional: confirm the replacement worked
!grep -n "clean_spotify_post2012" prepwork.py

!python prepwork.py

# Re-run setup libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset
file_path = "/content/clean_spotify_post2012.csv"
df = pd.read_csv(file_path)

# Target and features
y = df['track_popularity']
feature_cols = [
    'danceability', 'energy', 'musical_key', 'loudness', 'mode',
    'speechiness', 'acousticness', 'instrumentalness',
    'liveness', 'valence', 'tempo', 'time_signature'
]
X = df[feature_cols]

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train baseline models to recreate variables
lr_model = LinearRegression().fit(X_train, y_train)
dt_model = DecisionTreeRegressor(random_state=42).fit(X_train, y_train)

# Predictions for baseline models (needed later)
y_pred_lr = lr_model.predict(X_test)
y_pred_dt = dt_model.predict(X_test)

r2_lr = r2_score(y_test, y_pred_lr)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
r2_dt = r2_score(y_test, y_pred_dt)
rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))

print("Setup complete — ready for Random Forest ✅")

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)

r2_rf = r2_score(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
mae_rf = mean_absolute_error(y_test, y_pred_rf)

print("\n=== Random Forest ===")
print("R²:", round(r2_rf, 4))
print("RMSE:", round(rmse_rf, 4))
print("MAE:", round(mae_rf, 4))

# R² = 0.3416
# Explains 34.16% of the variation in track popularity. Higher is better.
# Much higher than Linear Regression’s ~0.15.
# Shows that Random Forest captures nonlinear patterns between features and popularity (like combinations of danceability + energy).

# RMSE = 16.23
# Average prediction error (in popularity points). Lower is better.
# Smaller than Decision Tree (≈ 23) and Linear Regression (≈ 18).
# Our predictions are closer to real popularity than before.

# MAE = 12.40
# Average absolute difference between prediction and actual. Lower is better.
# Similar trend — lower = more accurate.
# Confirms consistent improvement.

