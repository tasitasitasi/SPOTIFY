# -*- coding: utf-8 -*-
"""homework2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zIbS-STiv3lkHrCSxSPiIomSKK7sUpkB
"""

# loading libraries
import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
from sklearn.metrics import (
    mean_squared_error, r2_score, mean_absolute_error,
    accuracy_score, f1_score, classification_report, confusion_matrix
)
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

from google.colab import files
uploaded = files.upload()

# loading dataset
file_path = "clean_spotify_post2012.csv"
df = pd.read_csv(file_path)

print("Dataset shape:", df.shape)
print(df.head())

feature_cols = [
    'danceability', 'energy', 'musical_key', 'loudness', 'mode',
    'speechiness', 'acousticness', 'instrumentalness',
    'liveness', 'valence', 'tempo', 'time_signature'
]
target_col = 'track_popularity'

X = df[feature_cols]
y = df[target_col]

#Train/Test split (same split used for regression & classification)
X_train, X_test, y_train_reg, y_test_reg = train_test_split(
    X, y, test_size=0.2, random_state=42
)

#Baseline regression models
# Linear Regression
lr = LinearRegression().fit(X_train, y_train_reg)
y_pred_lr = lr.predict(X_test)
r2_lr = r2_score(y_test_reg, y_pred_lr)
rmse_lr = math.sqrt(mean_squared_error(y_test_reg, y_pred_lr))

#Decision Tree Regressor
dt_reg = DecisionTreeRegressor(random_state=42).fit(X_train, y_train_reg)
y_pred_dt = dt_reg.predict(X_test)
r2_dt = r2_score(y_test_reg, y_pred_dt)
rmse_dt = math.sqrt(mean_squared_error(y_test_reg, y_pred_dt))

#Improvement: Random Forest Regressor
rf = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_train, y_train_reg)
y_pred_rf = rf.predict(X_test)
r2_rf = r2_score(y_test_reg, y_pred_rf)
rmse_rf = math.sqrt(mean_squared_error(y_test_reg, y_pred_rf))
mae_rf = mean_absolute_error(y_test_reg, y_pred_rf)

print("=== Regression Summary ===")
print(f"Linear Regression     -> R²={r2_lr:.4f}, RMSE={rmse_lr:.2f}")
print(f"Decision Tree Regr.   -> R²={r2_dt:.4f}, RMSE={rmse_dt:.2f}")
print(f"Random Forest Regr.   -> R²={r2_rf:.4f}, RMSE={rmse_rf:.2f}, MAE={mae_rf:.2f}")

#Refinement: Categorize popularity
low_thresh = y_train_reg.quantile(0.33)
high_thresh = y_train_reg.quantile(0.66)

def to_popularity_class(val, low=low_thresh, high=high_thresh):
    if val <= low:
        return 'Low'
    elif val <= high:
        return 'Medium'
    else:
        return 'High'

y_train_cls = y_train_reg.apply(to_popularity_class)
y_test_cls  = y_test_reg.apply(to_popularity_class)

print("\n=== Class distribution (train/test) ===")
print(y_train_cls.value_counts(normalize=True).round(3))
print(y_test_cls.value_counts(normalize=True).round(3))

#Classification models
#Logistic Regression
log_reg_pipeline = Pipeline([
    ('scaler', StandardScaler(with_mean=False)),
    ('clf', LogisticRegression(max_iter=200, multi_class='auto', random_state=42))
]).fit(X_train, y_train_cls)

y_pred_log = log_reg_pipeline.predict(X_test)
acc_log = accuracy_score(y_test_cls, y_pred_log)
f1w_log = f1_score(y_test_cls, y_pred_log, average='weighted')

#Decision Tree Classifier
dt_clf = DecisionTreeClassifier(random_state=42).fit(X_train, y_train_cls)
y_pred_dt_clf = dt_clf.predict(X_test)
acc_dt = accuracy_score(y_test_cls, y_pred_dt_clf)
f1w_dt = f1_score(y_test_cls, y_pred_dt_clf, average='weighted')

print("\n=== Classification Summary ===")
print(f"Logistic Regression   -> Acc={acc_log:.4f},  F1_weighted={f1w_log:.4f}")
print(f"Decision Tree Class.  -> Acc={acc_dt:.4f},  F1_weighted={f1w_dt:.4f}")
print("\nLogReg report:\n", classification_report(y_test_cls, y_pred_log))
print("DT Class report:\n", classification_report(y_test_cls, y_pred_dt_clf))
print("Confusion Matrix (LogReg):\n", confusion_matrix(y_test_cls, y_pred_log))
print("Confusion Matrix (DT):\n", confusion_matrix(y_test_cls, y_pred_dt_clf))


#Visual – Baseline vs Improved
plt.figure(figsize=(10, 6))
y_pos = np.arange(len(df_comp))
plt.barh(y_pos, df_comp["Score"])
plt.yticks(y_pos, df_comp["Model"])
plt.xlabel("R² (Regression) / Accuracy (Classification)")
plt.title("Baseline vs Improvements — Model Performance")
for i, v in enumerate(df_comp["Score"]):
    plt.text((v + 0.01) if v >= 0 else (v - 0.15), i, f"{v:.2f}", va='center')
plt.tight_layout()
plt.show()

# Show numeric and percentage distribution
print("Class distribution (training set):")
print(y_train_cls.value_counts())
print("\nClass distribution (%):")
print(y_train_cls.value_counts(normalize=True).round(3) * 100)